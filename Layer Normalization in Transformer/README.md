# Layer Normalization

Layer normalization is a technique used to stabilize and accelerate the training of deep neural networks, particularly in models like transformers. It normalizes the inputs across the features of each layer to ensure that the activations have a consistent scale and distribution, which can help the model converge faster and more reliably.


![image](https://github.com/user-attachments/assets/eb6a3502-0c37-4b87-a650-e77151da33c2)


